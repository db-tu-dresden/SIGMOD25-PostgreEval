{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f7de08",
   "metadata": {},
   "source": [
    "# Experiment 07: Beyond Textbook Optimizers\n",
    "\n",
    "Process this notebook like so to generate the PDF output:\n",
    "\n",
    "```bash\n",
    "jupyter execute --inplace 05-Beyond-Textbook.ipynb\n",
    "jupyter nbconvert --to pdf --TagRemovePreprocessor.remove_cell_tags='{\"hide\"}' 05-Beyond-Textbook.ipynb\n",
    "```\n",
    "\n",
    "# Internals\n",
    "\n",
    "The cells in this section can be ignored in the PDF output. They perform the technical aspects of the data analysis.\n",
    "Please take a look at the following sections to see the actual outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849cb7",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import figure, ticker\n",
    "\n",
    "from postbound.db import postgres\n",
    "from postbound.experiments import workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7223b0e",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "results_base = Path(\"/ari/results/experiment-07-beyond-textbook/\")\n",
    "output_dir = Path(\"/ari/results/eval/experiment-07-beyond-textbook/\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "workloads.workloads_base_dir = \"/ari/postbound/workloads\"\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"ps.fonttype\"] = 42\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdcdc1",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def load_pg_explain(raw_explain: str) -> Optional[postgres.PostgresExplainPlan]:\n",
    "    plan_json = json.loads(raw_explain)\n",
    "    if not plan_json:\n",
    "        return None\n",
    "    return postgres.PostgresExplainPlan(plan_json)\n",
    "\n",
    "\n",
    "def read_df(workload: workloads.Workload) -> Optional[pd.DataFrame]:\n",
    "    data_file = results_base / workload.name.lower() / \"data-shift.csv\"\n",
    "    if not data_file.exists():\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(data_file, converters={\"query_plan\": load_pg_explain})\n",
    "    df[\"label\"] = pd.Categorical(\n",
    "        df[\"label\"], categories=workload.labels(), ordered=True\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_evolution_plots(\n",
    "    df: pd.DataFrame | None, *, workload: workloads.Workload\n",
    ") -> dict[str, figure.Figure]:\n",
    "    if df is None:\n",
    "        return {}\n",
    "\n",
    "    plots: dict[str, figure.Figure] = {}\n",
    "\n",
    "    for label in workload.labels():\n",
    "        current_samples = df.query(\"label == @label\").copy()\n",
    "        current_samples[\"plan_type\"] = current_samples[\"plan_type\"].map(\n",
    "            {\"native-fixed\": \"PG native\", \"robust-fixed\": \"Robust (UES)\"}\n",
    "        )\n",
    "        current_samples[\"db_size\"] = (current_samples[\"fill_ratio\"] / 0.6) * 100\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        g = sns.lineplot(\n",
    "            current_samples,\n",
    "            x=\"db_size\",\n",
    "            y=\"total_runtime\",\n",
    "            hue=\"plan_type\",\n",
    "            style=\"plan_type\",\n",
    "            markers=True,\n",
    "            dashes=False,\n",
    "            ax=ax,\n",
    "        )\n",
    "        g.axvline(100.0, color=\"grey\", linestyle=\":\")\n",
    "        g.set(\n",
    "            xlabel=\"Database size\",\n",
    "            ylabel=\"Runtime [s]\",\n",
    "            title=f\"{workload.name} query {label}\",\n",
    "        )\n",
    "        g.xaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        g.legend(title=\"Optimizer type\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        out_file = output_dir / workload.name.lower() / f\"data-shift-{label}.pdf\"\n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(out_file)\n",
    "        plt.close(fig)\n",
    "\n",
    "        plots[label] = fig\n",
    "\n",
    "    return plots\n",
    "\n",
    "\n",
    "def linear_approx(\n",
    "    fill_ratios: pd.Series, *, min_fr, max_fr, min_rt, max_rt\n",
    ") -> pd.Series:\n",
    "    m = (max_rt - min_rt) / (max_fr - min_fr)\n",
    "    n = min_rt - (m * min_fr)\n",
    "    return m * fill_ratios + n\n",
    "\n",
    "\n",
    "def determine_largest_jump(current_sample: pd.DataFrame) -> pd.Series:\n",
    "    current_sample = current_sample.sort_values(by=\"fill_ratio\").reset_index(drop=True)\n",
    "\n",
    "    min_fill_ratio, max_fill_ratio = (\n",
    "        current_sample[\"fill_ratio\"].min(),\n",
    "        current_sample[\"fill_ratio\"].max(),\n",
    "    )\n",
    "    min_rt = current_sample.query(\"fill_ratio == @min_fill_ratio\")[\n",
    "        \"total_runtime\"\n",
    "    ].item()\n",
    "    max_rt = current_sample.query(\"fill_ratio == @max_fill_ratio\")[\n",
    "        \"total_runtime\"\n",
    "    ].item()\n",
    "\n",
    "    approx_rts = linear_approx(\n",
    "        current_sample[\"fill_ratio\"],\n",
    "        min_fr=min_fill_ratio,\n",
    "        max_fr=max_fill_ratio,\n",
    "        min_rt=min_rt,\n",
    "        max_rt=max_rt,\n",
    "    )\n",
    "    rt_deviation = current_sample[\"total_runtime\"] - approx_rts\n",
    "\n",
    "    jump_point = rt_deviation.abs().idxmax()\n",
    "    expected_runtime = current_sample.at[jump_point, \"total_runtime\"]\n",
    "    actual_runtime = approx_rts.loc[jump_point]\n",
    "    jump_slowdown = max(expected_runtime, actual_runtime) / min(\n",
    "        expected_runtime, actual_runtime\n",
    "    )\n",
    "    jump_fill_ratio = current_sample.at[jump_point, \"fill_ratio\"]\n",
    "    return pd.Series(\n",
    "        dict(\n",
    "            jump_point=jump_fill_ratio,\n",
    "            slowdown=jump_slowdown,\n",
    "            min_rt=min_rt,\n",
    "            max_rt=max_rt,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def make_jumps_df(df: pd.DataFrame | None) -> Optional[pd.DataFrame]:\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    jumps_df = (\n",
    "        df.groupby([\"plan_type\", \"label\"], as_index=False, observed=True)\n",
    "        .apply(determine_largest_jump, include_groups=False)\n",
    "        .assign(rt_diff=lambda samples: samples[\"max_rt\"] - samples[\"min_rt\"])\n",
    "        .sort_values(by=[\"label\", \"plan_type\"])\n",
    "    )\n",
    "\n",
    "    return jumps_df\n",
    "\n",
    "\n",
    "def select_underest_jump_replacement(\n",
    "    df: pd.DataFrame | None,\n",
    "    *,\n",
    "    workload: workloads.Workload,\n",
    "    plots: dict[str, figure.Figure],\n",
    ") -> Optional[figure.Figure]:\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    underest_results = df.query(\"rt_diff >= 1 & slowdown >= 1.3 & jump_point >= 0.6\")\n",
    "    if underest_results.empty:\n",
    "        warnings.warn(\n",
    "            f\"No jumps during underestimation detected for workload {workload.name}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    selected: str = underest_results.iloc[underest_results[\"rt_diff\"].argmax()][\"label\"]\n",
    "    return plots[selected]\n",
    "\n",
    "\n",
    "def select_overest_jump_replacement(\n",
    "    df: pd.DataFrame | None,\n",
    "    *,\n",
    "    workload: workloads.Workload,\n",
    "    plots: dict[str, figure.Figure],\n",
    ") -> Optional[figure.Figure]:\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    underest_results = df.query(\"rt_diff >= 1 & slowdown >= 1.3 & jump_point < 0.6\")\n",
    "    if underest_results.empty:\n",
    "        warnings.warn(\n",
    "            f\"No jumps during overestimation detected for workload {workload.name}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    selected: str = underest_results.iloc[underest_results[\"rt_diff\"].argmax()][\"label\"]\n",
    "    return plots[selected]\n",
    "\n",
    "\n",
    "def determine_cout_rt_increases(sample: pd.DataFrame) -> pd.DataFrame:\n",
    "    sample = sample.sort_values(by=\"fill_ratio\").reset_index(drop=True)\n",
    "    rt_increases = sample[\"total_runtime\"].shift(-1) - sample[\"total_runtime\"]\n",
    "    cout_increases = sample[\"c_out\"].shift(-1) - sample[\"c_out\"]\n",
    "    return pd.DataFrame(\n",
    "        dict(\n",
    "            fill_ratio=sample[\"fill_ratio\"],\n",
    "            runtime_increase=rt_increases,\n",
    "            cout_increase=cout_increases,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def make_cout_rt_corr_df(df: pd.DataFrame | None) -> Optional[pd.DataFrame]:\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"c_out\"] = df[\"query_plan\"].map(\n",
    "        lambda plan: plan.total_processed_rows() if plan else np.nan\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        df.groupby(\"label\", observed=True)\n",
    "        .apply(determine_cout_rt_increases, include_groups=False)\n",
    "        .reset_index(drop=False)  # restore our labels\n",
    "        .drop(columns=\"level_1\")\n",
    "    )\n",
    "\n",
    "\n",
    "def make_cout_rt_corr_plot(\n",
    "    df: pd.DataFrame | None,\n",
    "    *,\n",
    "    jumps_df: pd.DataFrame | None,\n",
    "    workload: workloads.Workload,\n",
    ") -> Optional[figure.Figure]:\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    jumping_queries = jumps_df.query(\"slowdown >= 1.3 & rt_diff >= 1\")[\"label\"].unique()\n",
    "    jumping_queries = set(jumping_queries)\n",
    "\n",
    "    corr_df = df.query(\"label.isin(@jumping_queries)\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    g = sns.scatterplot(corr_df, x=\"cout_increase\", y=\"runtime_increase\", ax=ax)\n",
    "    g.set_xscale(\"log\")\n",
    "    g.set_yscale(\"log\")\n",
    "    g.set(xlabel=\"$C_{out}$ increase [tuples]\", ylabel=\"Runtime increase [s]\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_file = output_dir / workload.name.lower() / \"cout-rt-corr.pdf\"\n",
    "    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_file)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def jump_count_summary(df: pd.DataFrame | None) -> Optional[pd.DataFrame]:\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"plan_type\"] = df[\"plan_type\"].map(\n",
    "        {\"native-fixed\": \"PG native\", \"robust-fixed\": \"Robust (UES)\"}\n",
    "    )\n",
    "    relevant_queries = (\n",
    "        df.query(\"rt_diff >= 1\").groupby(\"plan_type\", as_index=True).size()\n",
    "    )\n",
    "    jumping_queries = (\n",
    "        df.query(\"rt_diff >= 1 & slowdown >= 1.3\")\n",
    "        .groupby(\"plan_type\", as_index=True)\n",
    "        .size()\n",
    "    )\n",
    "    results = pd.DataFrame(\n",
    "        {\"relevant_queries\": relevant_queries, \"jumping_queries\": jumping_queries},\n",
    "        index=relevant_queries.index,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def jump_pos_summary(df: pd.DataFrame | None) -> Optional[np.float64]:\n",
    "    jump_pos = (\n",
    "        df.query(\"rt_diff >= 1 & slowdown >= 1.3\")\n",
    "        .assign(\n",
    "            jump_pos=lambda sample: np.where(\n",
    "                sample[\"jump_point\"] <= 0.6, \"overest\", \"underest\"\n",
    "            )\n",
    "        )\n",
    "        .groupby(\"jump_pos\")\n",
    "        .size()\n",
    "    )\n",
    "    underest_share = 1 - jump_pos.loc[\"underest\"] / jump_pos.sum()\n",
    "    return underest_share\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812df1f",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "job = workloads.job()\n",
    "stats = workloads.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a996b6a",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "df_job = read_df(job)\n",
    "df_stats = read_df(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbd24d",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "job_plots = make_evolution_plots(df_job, workload=job)\n",
    "_ = make_evolution_plots(df_stats, workload=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f3fe3",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "jumps_job = make_jumps_df(df_job)\n",
    "jumps_stats = make_jumps_df(df_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6db03c",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "cout_rt_corr_job = make_cout_rt_corr_df(df_job)\n",
    "cout_rt_corr_stats = make_cout_rt_corr_df(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53669e",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "corr_plot_job = make_cout_rt_corr_plot(\n",
    "    cout_rt_corr_job, jumps_df=jumps_job, workload=job\n",
    ")\n",
    "_ = make_cout_rt_corr_plot(cout_rt_corr_stats, jumps_df=jumps_stats, workload=stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbca0d",
   "metadata": {},
   "source": [
    "# Suggested replacement plots\n",
    "\n",
    "These are the plots that might be the best replacements for the original plots based on the current hardware. See the README\n",
    "for the motivation behind this strategy.\n",
    "\n",
    "Please note that these plots are determined automatically using a coarse heuristic. In contrast, the plots in the original\n",
    "paper have been selected semi-automatically using the same statistics but with human oversight. If one of the plots shows a\n",
    "significant difference, please take a look at the remaining plots for the other queries. Perhaps the heuristic selected a poor\n",
    "replacement plot. In our testing, we saw this happen a couple of times.\n",
    "The individual plots are stored at `results/eval/experiment-07-beyond-textbook` in the Docker volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7287856",
   "metadata": {},
   "source": [
    "**Replacement for Figure 12(a):** This plot should show a sharp increase in query runtime somewhere in the overestimation\n",
    "portion (i.e. database size < 100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_overest_jump_replacement(jumps_job, workload=job, plots=job_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3903771",
   "metadata": {},
   "source": [
    "**Replacement for Figure 12(b):** This plot should show a sharp increase in query runtime somewhere in the underestimation portion (i.e. database size > 100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_underest_jump_replacement(jumps_job, workload=job, plots=job_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea89b51",
   "metadata": {},
   "source": [
    "**Replacement for Figure 14:** This plot should show a roughly linear correlation between $C_{out}$ increase and runtime\n",
    "increase, especially for larger values of $C_{out}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b04741",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_plot_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9888010",
   "metadata": {},
   "source": [
    "# Aggregated statistics\n",
    "\n",
    "**Table 4:** Number of relevant queries and jumping queries.\n",
    "\n",
    "Relevant queries are those whose runtime differs by at least 1 second. Jumping queries are selected from all relevant queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678adaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jump_count_summary(jumps_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269c461",
   "metadata": {},
   "source": [
    "We also calculate where the majority of jumps happen - while underestimating the true cardinalities or while overestimating\n",
    "them.\n",
    "We report the percentage of queries that jump during the underestimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "jump_pos_summary(jumps_job)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
